{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Link to the course module: https://course18.fast.ai/lessonsml1/lesson1.html\n",
    "* Link to forum discussing location of ml1 (fastai-v1) course: https://forums.fast.ai/t/where-is-machine-learning-repo/83869\n",
    "* Command to install `fastai-v1` library : conda install -c pytorch -c fastai fastai=1.0.61\n",
    "* To solve problem with installing `bcolz`, install new python 3.8 environment but then use `conda install bcolz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 | Basics - Introduction to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. About autoreload"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-18T16:33:56.011483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip uninstall opencv-contrib-python\n",
    "!pip uninstall seaborn\n",
    "!pip uninstall bcolz\n",
    "!pip uninstall graphviz\n",
    "!pip uninstall sklearn_pandas\n",
    "!pip uninstall isoweek\n",
    "!pip uninstall ipywidgets\n",
    "!pip uninstall tqdm\n",
    "!pip uninstall -U autopep8   \n",
    "!pip uninstall pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T16:33:03.724882Z",
     "start_time": "2024-12-18T16:33:03.720381Z"
    }
   },
   "source": [
    "# !pip install opencv-contrib-python\n",
    "# !pip install seaborn\n",
    "# !pip install bcolz\n",
    "# !pip install graphviz\n",
    "# !pip install sklearn_pandas\n",
    "# !pip install isoweek\n",
    "# !pip install ipywidgets\n",
    "# !pip install tqdm\n",
    "# !pip install -U autopep8   \n",
    "# !pip install pyarrow"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These two lines help modify modules and update on demand\n",
    "* Mainly to show tables within the notebook itself\n",
    "* If you want to use any Python variable inside a Jupyter console, use CURLIES"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T16:33:06.135865Z",
     "start_time": "2024-12-18T16:33:05.769656Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T16:33:06.385791Z",
     "start_time": "2024-12-18T16:33:06.142780Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"../fastai/\")\n",
    "from structured import *\n",
    "from imports import *\n",
    "\n",
    "os.chdir(currDir)\n",
    "# ____________________________________________________________ #\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "# ____________________________________________________________ #"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bcolz'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m currDir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetcwd()\n\u001B[0;32m      4\u001B[0m os\u001B[38;5;241m.\u001B[39mchdir(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../fastai/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstructured\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimports\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      8\u001B[0m os\u001B[38;5;241m.\u001B[39mchdir(currDir)\n",
      "File \u001B[1;32mD:\\MAINPAPERWORK\\CODE_PROJECTS\\DATA_SCIENCE\\FastAI\\fastai\\structured.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimports\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn_pandas\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataFrameMapper\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LabelEncoder, StandardScaler\n",
      "File \u001B[1;32mD:\\MAINPAPERWORK\\CODE_PROJECTS\\DATA_SCIENCE\\FastAI\\fastai\\imports.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeepreload\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m reload \u001B[38;5;28;01mas\u001B[39;00m dreload\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mos\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mmath\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mcollections\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mthreading\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mbcolz\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mrandom\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mpickle\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01msys\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mitertools\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mstring\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01msys\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mre\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mdatetime\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'bcolz'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. About Software development practices in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Pep 8` is the Python standard for developing source code in Python\n",
    "* Most of DS/ML is `prototyping model`\n",
    "* The component of software development is taken away (although very loosely)\n",
    "* Best practices for prototyping model is barely even standardised, so you are on your own to setup your own style and standards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. About Blue Book for Bulldozers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is about predicting `sales price` for heavy equipments\n",
    "* here you better keep a seperate `PATH` variable to store the location of the dataset\n",
    "* Also the *structured* module from fastai has been removed to *tabular*\n",
    "* This is an example of a `structured` data --> this is where pandas come in\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2DATA = '../datasets/kaggle/bluebook_bulldozers/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Basics on Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use the `head` command to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head '../datasets/kaggle/bluebook_bulldozers/Train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Or use the pandas library\n",
    "* the `f{PATH}Train.csv` is sth known as the Python 3.7 string\n",
    "* e.g. if you have variable NAME = 'deedo' , and then you type in command `f My name is {NAME}`, then this will print `My name is deedo`\n",
    "* But keep in mind if you remove `f` it WONT WORK, `f` allows `interpolation` of string variables\n",
    "* You can also make it work for `integers` use code inside curlies for example `{NAME}.upper()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting low_memory to true just saves ram. setting to false lets it read more of the file\n",
    "df_raw = pd.read_csv(f'{PATH2DATA}Train.csv',\n",
    "                     low_memory=False, parse_dates=[\"saledate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You will find that pandas `DataFrame` are exactly like R dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Visualizing your Data to Inspect Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Important to look at your data and understand `the format`, `how stored`, `what type of values` it holds\n",
    "* Even if you read the description of the data, it will never be enough\n",
    "* `.tail()` will show us the last few rows\n",
    "* Keep in mind dont look at the data for too long, it risks `overfitting` \n",
    "* You should generally keep the `EDA` right towards the end (EDA means analysing the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000):\n",
    "        with pd.option_context(\"display.max_columns\", 1000):\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.tail().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Inspecting the Loss-Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quite oftenly it is important to check what loss functions the data uses\n",
    "* It may just fall into the pre-processing stage\n",
    "* In Kaggle, you can check this by going on the `evaluation` section\n",
    "* For e.g. in Bluebook for Bulldozers we evaluate using `Root Means Square Log Error (RMLSE)`\n",
    "* Hence we need to log out our `dependant variables` which is the sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.SalePrice = np.log(df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can inspect each column of the DataFrame, as a seperate pandas `series`\n",
    "* These panda series can easily be transferred to `numpy arrays`\n",
    "* Hence numpy and pandas have a harmonious dev environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Curse of Dimenionality and No Free Lunch Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, there is this essentially the belief in 90s theoreticians in ML that ...\n",
    "* ... data points gets `stretched` to the edges of each dimenion you add to the data\n",
    "* Hence the distance between them, which is essen. a `feature` becomes redundant\n",
    "* Emperical modern ML agrees otherwise, where Jeremy says that adding more columns improves the model's performance\n",
    "* Just because they are on edge, they still varies\n",
    "* Even kNN works better on high-dimensional data, and it relies on distance between points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now the No Free Lunch --> \"there is NO TYPE of model that WORKS WELL for any TYPE of dataset\"\n",
    "* Makes sense in Math, any random dataset will not be able to be approachable from point of view of methods that can make sense of random dataset\n",
    "* Why ? because models we use rely on `features` to make sense of data\n",
    "* A random dataset has no `features`, unless you know the type of process that generated that data well in hand but then it makes no sense to use ML if you already know the answer\n",
    "* In the real applications, data is `NOT RANDOM`\n",
    "* Mathematically, we believe that the real data sits on a `lower dimensional manifold`\n",
    "* But it was created by some `Causal Structure` that is of stochastic nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Initialise the Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regressor is used for fitting data for `continuous variables` that is dependant\n",
    "* Classifier for `discrete variables` or `categorical variables` that is dependant\n",
    "* RFAlgorithm is best for beginings, as if your data doesnt work with RF to begin with, there is possibly sth already wrong with it\n",
    "* In the `.fit` method you pass in the `independant variable`, that is the things you are going to be using to predict, in our case we set 'SalePrice' meaning we want to use EVERYTHING EXCEPT SALEPRICE\n",
    "* The `pd.drop` function brings a new DataFrame with the `SalePrice` column REMOVED (it can work with rows as well FYI)\n",
    "* And then the `dependant variable` that is the things you want to predict, for us this `df_raw.SalePrice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(df_raw.drop('SalePrice', axis=1), df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. How to Pre-Process (Convert all Colms to Numbers) Before Using RF Algorithm as Part of Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can tell from previous cell call that our datas contains `NON-NUMBERS`\n",
    "* Dataset bluebook contains both `continuous` (numeric) and `categorical` (non-numeric are partially numeric like ZipCode)\n",
    "* One example of non-numreic data in bluebook is the `Sale Date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.saledate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can the **dtype** of this entry is **datetime64bits**\n",
    "* Not usefull when you have to do RF algorithm\n",
    "* Now we are going to do our very first `feature engineering`\n",
    "* You need to **transform** the data information that you have into something that is **usefull** to you\n",
    "* For example, was there christmas on the day, if you are analyse sport, was it a football match that date\n",
    "* No ML algorithm can tell you that, you have to use **feature engineering** to make use of the useless data presented to you\n",
    "* Now using `add_datepart` will allow you to **split** the date into Year, Month, Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(df_raw, 'saledate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now if you inspect all the fields of your data you see that the saledate is gone\n",
    "* and now addittional fields have appeared known as **saleYear**, **saleMonth**, **saleWeek**, and **saleDay** and many more ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is the most basic type of `feature engineering`\n",
    "* `[BONUS]` In your spare time, try to do this step manually so you can understand the pre-processing properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10. Feature Engineering :2 -- Take care of the Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see below, you still have certain columns that contain **Strings**\n",
    "* How do you get rid of those ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use `train_cats` method in fastai library\n",
    "* It basically converts strings in defined order to integers\n",
    "* But `WARNING` make sure to use the same defined order for both **TRAIN** and **TEST** and **VAL** data\n",
    "* Otherwise your model will be **non-predictive**\n",
    "* For that issue you can instead use the `apply_cats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After running `train_cats` you see nothing changed, because it does it behind the scenes\n",
    "* But now you can see addittional modules appear for UsageBand\n",
    "* In particularly u can see UsageBand.`cat.categories` appear\n",
    "* This method will list all the categories to *numerify*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also you notice from above that the list is in a wierd order, `High, Low, Medium`\n",
    "* Truth is it doesnt matter **TOO MUCH** when you make decision trees\n",
    "* Because decision trees can split things at certain points ...\n",
    "* ... for example you'd have trees that might say well lets compare points *high* vs *low* and *medium*\n",
    "* ... or *medium* vs *high* and *low*\n",
    "* That is weird for orderly fashion sense\n",
    "* So instead we predefine manually this order in the following way, it wont improve the performance too much but just a tad boost helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.categories(\n",
    "    ['High', 'Medium', 'Low'], ordered=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also you can access the train_cats codes for the UsageBand by using the `codes` method\n",
    "* You can see it has assigned `Low`-> 1,  `High` -> 0,  `Medium` -> 2, and `NaN` -> -1\n",
    "* Keep in mind that Random Forest algorithm consits of trees that make a ...\n",
    "* ... a `single split` based on the `threshold`, e.g. you could potentially split with a condition that if it is less than or greater than 1 to compare `HIGH VS MEDIUM n LOW`\n",
    "* And if it is less than or greater than 2 we compare `LOW VS HIGH n MEDIUM`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The UsageBand column data is what is known as `ordinal` type of `categorical data`\n",
    "* THat it follows an order for example, *high*, *medium*, or *low*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11. FeatureEng. :3 -- Take Care of Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue from `1:03:52`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('airdmp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dd1fd71c541aceb5a112498f246b8d31e870cb987bd8e84920daf433ebc4fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
