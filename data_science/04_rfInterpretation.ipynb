{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Actual coding begins at `17:39`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Interpretation - Chapter 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `set_rf_samples` means how many of the samples are made from the tree\n",
    "* Before we start making trees we have two choiced\n",
    "    * Sample w replacement from the entire dataset\n",
    "    * Subsampling from the dataset \n",
    "* In the latter, the trees are made from only a small variation of the set \n",
    "* This is a trick oftenly done when dataset is very large\n",
    "* The subsamples are also sometimes called `bootstrap samples`\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On `growth scale` of rf, consider the size to be $\\log_2 (set\\ rf\\ samples)$\n",
    "* The `no. of leaf nodes` is equal to the set_rf_samples\n",
    "* Hence there is a `linear relationship` between set_rf_samples and number of leaf nodes\n",
    "* So, in a sense, number of rf samples also decides the number of decisions made by the rf\n",
    "* Therefore, the RF is going to be `less rich` in what it can predict as it will make `less binary choices` \n",
    "* How this relates to overfitting ? --> basically having low rfsamples will mean `less chances` of `overfitting`\n",
    "* But it also means each of the individual tree in the forest will be `less accurate`\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now looking in-depth what the idealogy about models with `bagging` is \n",
    "* You are trying to do two things:\n",
    "    * A) Each individual estimator is as accurate as possible $\\uparrow$ on the training set\n",
    "    * B) The correlation between the estimators is low as possible $\\downarrow$\n",
    "    * So when you `average them out` together you end up with `better generalization`\n",
    "* Hence, by setting set_rf_samples with a low number, you are decreasing the `A` factor and increasing the `B` factor\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now what happens when you set `oob_score` to True\n",
    "* In this case, remind yourself that there is these `residual` rows that did'net get included in the training set after the `subsampling stage`\n",
    "* You can essentially construct a `quasi` validation set from this\n",
    "* Obviously if you do not prefer this, it is possible to use `reset_rf_samples()` which simply sets rfsamples to 0 and uses the entire dataset, you WONT be able to use `oob_score` anymore now!\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nextup is `min_samples_leaf` , setting this from (for eg) from 1 to 2, means that the depth of the decision tree will be `subtracted by 1`\n",
    "* Because everytime we `double` the min_samples_leaf, we are removing `one layer` from the forest\n",
    "* And the number of leaves will be `halved` if min_samples_leaf is `doubled`\n",
    "* In this case, increasing min_samples_leaf will decrease `(A)` and increase `(B)` which `might` help us from `overfitting`\n",
    "* Ideal choices for min_samples_leaf can be: *1, 3, 5, 10, 25, 100*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, the `max_features` determines how much portion of the features are selected `per-split`\n",
    "* So if max_feautures = 0.5, then at each split, we take 0.5 of the features\n",
    "* This will `reduce` the `coorelation` between the individual trees and *MAY* help with overfitting \n",
    "* The trade-off is that each of the tree will be `less accurate`\n",
    "* Options you can have for `max_features` is :\n",
    "    > sqrt for allow the sqrt of features\n",
    "\n",
    "    > log2 to allow log2 of the number of features set\n",
    "    \n",
    "    > None means have all of them available at each split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Modules Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"../fastai/\")\n",
    "from structured import *       \n",
    "from imports import *\n",
    "os.chdir(currDir)\n",
    "# ____________________________________________________________ #\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "# ____________________________________________________________ #\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2DATA = \"../datasets/kaggle/bluebook_bulldozers/\"\n",
    "# !dir \"../datasets/kaggle/corporcion_favorita_grocery_sales/\"\n",
    "\n",
    "# below is just the param to control different features in a graph plot\n",
    "set_plot_sizes(12, 14, 16)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f'{PATH2DATA}Train.csv',\n",
    "                     low_memory=False, parse_dates=[\"saledate\"])\n",
    "\n",
    "# convert the columns to log\n",
    "df_raw.SalePrice = np.log(df_raw.SalePrice)\n",
    "\n",
    "# extract timeOfDay, timeOfMonth etc from time and date\n",
    "add_datepart(df_raw, 'saledate')\n",
    "\n",
    "# categorical to numeric - partly\n",
    "train_cats(df_raw)\n",
    "df_raw.UsageBand.cat.set_categories(\n",
    "    [\"High\", \"Medium\", \"Low\"], ordered=True, inplace=True)\n",
    "\n",
    "# use proc_df to quantify string columns\n",
    "df_train, y_train, _ = proc_df(df_raw, 'SalePrice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a, n):\n",
    "    \"\"\"\n",
    "    a: number of samples (i.e. the entire dataset)\n",
    "    n: number of training set to split\n",
    "    \"\"\"\n",
    "    # a[:n] will retrieve the first (N - n_valid) rows for TRAINING set\n",
    "    # a[n:] will retirve the last (N - n_valid) rows got VALIDATION set\n",
    "    return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 12000\n",
    "\n",
    "# the number of training sets will be len(df) - n_valid\n",
    "n_trn = len(df_train) - n_valid\n",
    "\n",
    "# now split the entire dataset into training and validation\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "\n",
    "# before were raw, now get the real ones based on pre-processed version\n",
    "X_train, X_valid = split_vals(df_train, n_trn)\n",
    "y_train, y_valid = split_vals(y_train, n_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (389125, 66), y_train shape : (389125,),  x_valid shape : (12000, 66)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: {}, y_train shape : {},  x_valid shape : {}\".format(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airdmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d07b0008baf96b10859126fdec4e07f87ea8cd98c5be91794f454136925afeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
