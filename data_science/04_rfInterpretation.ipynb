{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Actual coding begins at `17:39`\n",
    "* Updated version of this lesson available at :: https://www.kaggle.com/code/ailobe/fastai-ml1-lesson2-rf-interpretation/notebook\n",
    "* The command `git restore .` will remove any changes and allow fresh pull from repo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Interpretation - Chapter 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Pre-Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `set_rf_samples` means how many of the samples are made from the tree\n",
    "* Before we start making trees we have two choiced\n",
    "    * Sample w replacement from the entire dataset\n",
    "    * Subsampling from the dataset \n",
    "* In the latter, the trees are made from only a small variation of the set \n",
    "* This is a trick oftenly done when dataset is very large\n",
    "* The subsamples are also sometimes called `bootstrap samples`\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On `growth scale` of rf, consider the size to be $\\log_2 (set\\ rf\\ samples)$\n",
    "* The `no. of leaf nodes` is equal to the set_rf_samples\n",
    "* Hence there is a `linear relationship` between set_rf_samples and number of leaf nodes\n",
    "* So, in a sense, number of rf samples also decides the number of decisions made by the rf\n",
    "* Therefore, the RF is going to be `less rich` in what it can predict as it will make `less binary choices` \n",
    "* How this relates to overfitting ? --> basically having low rfsamples will mean `less chances` of `overfitting`\n",
    "* But it also means each of the individual tree in the forest will be `less accurate`\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now looking in-depth what the idealogy about models with `bagging` is \n",
    "* You are trying to do two things:\n",
    "    * A) Each individual estimator is as accurate as possible $\\uparrow$ on the training set\n",
    "    * B) The correlation between the estimators is low as possible $\\downarrow$\n",
    "    * So when you `average them out` together you end up with `better generalization`\n",
    "* Hence, by setting set_rf_samples with a low number, you are decreasing the `A` factor and increasing the `B` factor\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now what happens when you set `oob_score` to True\n",
    "* In this case, remind yourself that there is these `residual` rows that did'net get included in the training set after the `subsampling stage`\n",
    "* You can essentially construct a `quasi` validation set from this\n",
    "* Obviously if you do not prefer this, it is possible to use `reset_rf_samples()` which simply sets rfsamples to 0 and uses the entire dataset, you WONT be able to use `oob_score` anymore now!\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nextup is `min_samples_leaf` , setting this from (for eg) from 1 to 2, means that the depth of the decision tree will be `subtracted by 1`\n",
    "* Because everytime we `double` the min_samples_leaf, we are removing `one layer` from the forest\n",
    "* And the number of leaves will be `halved` if min_samples_leaf is `doubled`\n",
    "* In this case, increasing min_samples_leaf will decrease `(A)` and increase `(B)` which `might` help us from `overfitting`\n",
    "* Ideal choices for min_samples_leaf can be: *1, 3, 5, 10, 25, 100*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, the `max_features` determines how much portion of the features are selected `per-split`\n",
    "* So if max_feautures = 0.5, then at each split, we take 0.5 of the features\n",
    "* This will `reduce` the `coorelation` between the individual trees and *MAY* help with overfitting \n",
    "* The trade-off is that each of the tree will be `less accurate`\n",
    "* Options you can have for `max_features` is :\n",
    "    > sqrt for allow the sqrt of features\n",
    "\n",
    "    > log2 to allow log2 of the number of features set\n",
    "    \n",
    "    > None means have all of them available at each split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries and Modules Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"../fastai/\")\n",
    "from structured import *       \n",
    "from imports import *\n",
    "os.chdir(currDir)\n",
    "# ____________________________________________________________ #\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "# ____________________________________________________________ #\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2DATA = \"../datasets/kaggle/bluebook_bulldozers/\"\n",
    "# !dir \"../datasets/kaggle/corporcion_favorita_grocery_sales/\"\n",
    "\n",
    "# below is just the param to control different features in a graph plot\n",
    "set_plot_sizes(12, 14, 16)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load dataset and Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f'{PATH2DATA}Train.csv',\n",
    "                     low_memory=False, parse_dates=[\"saledate\"])\n",
    "\n",
    "# convert the columns to log\n",
    "df_raw.SalePrice = np.log(df_raw.SalePrice)\n",
    "\n",
    "# extract timeOfDay, timeOfMonth etc from time and date\n",
    "add_datepart(df_raw, 'saledate')\n",
    "\n",
    "# categorical to numeric - partly\n",
    "train_cats(df_raw)\n",
    "df_raw.UsageBand.cat.set_categories(\n",
    "    [\"High\", \"Medium\", \"Low\"], ordered=True, inplace=True)\n",
    "\n",
    "# use proc_df to quantify string columns\n",
    "df_train, y_train, _ = proc_df(df_raw, 'SalePrice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a, n):\n",
    "    \"\"\"\n",
    "    a: number of samples (i.e. the entire dataset)\n",
    "    n: number of training set to split\n",
    "    \"\"\"\n",
    "    # a[:n] will retrieve the first (N - n_valid) rows for TRAINING set\n",
    "    # a[n:] will retirve the last (N - n_valid) rows got VALIDATION set\n",
    "    return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 12000\n",
    "\n",
    "# the number of training sets will be len(df) - n_valid\n",
    "n_trn = len(df_train) - n_valid\n",
    "\n",
    "# now split the entire dataset into training and validation\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "\n",
    "# before were raw, now get the real ones based on pre-processed version\n",
    "X_train, X_valid = split_vals(df_train, n_trn)\n",
    "y_train, y_valid = split_vals(y_train, n_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (389125, 66), y_train shape : (389125,),  x_valid shape : (12000, 66)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: {}, y_train shape : {},  x_valid shape : {}\".format(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Model Setup and Initial Run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Some pre-defined functions for output and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will take the RMSE\n",
    "def rmse(pred, known):\n",
    "    return np.sqrt(((pred-known)**2).mean())\n",
    "\n",
    "# function to round ans. to 5dp like Kaggle leaderboard answers\n",
    "\n",
    "\n",
    "def rounded(value):\n",
    "    return np.round(value, 5)\n",
    "\n",
    "# function to return the rmse scores and R^2 values for train and validation set\n",
    "\n",
    "\n",
    "def print_scores(model):\n",
    "    RMSE_train = rmse(model.predict(X_train), y_train)\n",
    "    RMSE_valid = rmse(model.predict(X_valid), y_valid)\n",
    "    R2_train = model.score(X_train, y_train)\n",
    "    R2_valid = model.score(X_valid, y_valid)\n",
    "\n",
    "    # list the scores and check if oob_score is present\n",
    "    scores = [rounded(RMSE_train), rounded(RMSE_valid),\n",
    "              rounded(R2_train), rounded(R2_valid)]\n",
    "    if hasattr(model, 'oob_score_'):\n",
    "        scores.append(model.oob_score_)\n",
    "    print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Subsampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interpretation of the model has less to do with getting the `best accuracy` and more into the `insights` regarding the data\n",
    "* In other words, how are the features within the data `correlated`\n",
    "* For this to be tested, the model must first be `reliable`\n",
    "* But also, when subsampling we need to make sure the subsample is `large enough` so that the model is reliable\n",
    "* For this example we use about 50000 samples\n",
    "* Recall that `oob_score=True` is only used when dataset is big enough to allow four split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_rf_samples(5000)   # the old set_rf_samples is causing error\n",
    "\n",
    "# first run of the model\n",
    "model1st = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n",
    "                                 n_jobs=-1, oob_score=True, random_state=17190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=0.5, min_samples_leaf=3, n_estimators=40,\n",
       "                      n_jobs=-1, oob_score=True, random_state=17190)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model and measure time taken\n",
    "%time model1st.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 One Hot Encode (@ 41:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Lecture Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Earlier we had converted `UsageBands` to \n",
    "    * High $\\rightarrow$ 0\n",
    "    * Low $\\rightarrow$ 1\n",
    "    * Medium $\\rightarrow$ 2\n",
    "* Obv. RF doesnt know the categories, it just sees 0,1 and 2\n",
    "* Now we can get about two or three `nested` splits here\n",
    "* But lets say, for sake of argument, we had a wider range, with columns such as Very Low, Very Medium, Very High or Unknown\n",
    "* Have a larger `range` of columns would `increase` the `number of splits`\n",
    "* This is inefficient as the everytime we do a nested split, we are `halving` the amount of data\n",
    "* Instead of this we can split the individual columns into `binary format` for e.g. say *isVeryLow*, *isVeryMedium*, or *isVeryHigh*\n",
    "* And this essentially allows us to reduce the number of `nested splits` which is ideal for efficiency down to 1\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is known as `one-hot encoding` of data points\n",
    "* ANd it is fine if we have too many columns being too similar since `linear models hate co-linearity` \n",
    "* In our case it is not that big of a deal\n",
    "* We are mainly doing one-hot encoding for the sake of `interpreting` your ML model\n",
    "* Sometimes it might reveal the true `influence/importance` of a feature that was latent when the nested splits could not capture the `true insight` of its influence\n",
    "* One-hot encoding is performed by pandas using the `pd.get_dummies` $\\rightarrow$ you can get more info on this using `??numericalize`\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is mainly implemented by setting the `max_cats` argument in `proc_df`\n",
    "* This argument decides if the limiting number of columns has `cardinality` that must be less than max_cats\n",
    "* For example, UsageBands has Low, Medium and High, i.e. cardinality = 3, Sex has Male, Female i.e. cardinality = 2\n",
    "* So here if we set max_cats = 7, all of those with cardinality less than 7 will be one-hot encoded\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Awkward question asked `@53:02 - @53:35`, actually not so awkward :D ... the question is that some data will be organised very orderly by lets say having a `grading system` \n",
    "* A grading system for example saying values shift from poor, to good, to very good\n",
    "* Then, using dummy variables (one-hot encoding) might destroy this order, how do we overcome it ?\n",
    "\n",
    "[ANS] You can easily make it an integer to prevent destruction of the order with `proc_df` by equalling it to its `cat.codes`, for example **df_raw.UsageBands = df_raw.UsageBands.cat.codes**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Removing Redundant Features (@ 54:50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Lecture Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This builds up from hierarchial sorting of importance\n",
    "* You can remove these features with the help of `dendograms`\n",
    "* This is a type of `heirarchical clustering` algorithm\n",
    "* Cluster analysis allows us to look at rows or columns and decide which ones are similar\n",
    "* A good example of cluster analysis is `k-means`\n",
    "* In heirarchical or anglomerative clustering, we look at every `pair` of `objects/points`\n",
    "* And then decide which two objects are the closest\n",
    "* Given those, delete them and replace them with an average that sits in the middle of those points\n",
    "* Then iteratively perform this `pairwise combining` that is taking a pair of points and replacing them with their averages\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In our example, we instead of looking at pair of points/objects, we look at the pair of `columns` and/or `variables`\n",
    "* We want to know which two `tree variables` are the most similar\n",
    "* The horizonal axis of the dendogram shows how similar are the two variables being compared\n",
    "* If the vertical line is more to right, the variables are more similar\n",
    "* In this particular example we use the units of `Spearman's R` to tell the difference between the varaibles\n",
    "* `Correlation coeff.` are almost similar to `R^2`, except correlation is between two variables, and R^2 is between the variable and its prediction\n",
    "* ALso we instead of comparing the points directly, we compare their `rank`, this will help fortify our `linearity assumption` when testing the correlation between variables\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After that we get the (@ 1:04:33) `out of band score`\n",
    "* Here it does rf on some dataframe and get the oob_score on that\n",
    "* The idea is to compare the effect on the oob_score_ after removing some of the variables one at a time\n",
    "* First you get a baseline oob_score_ by training on the entire data frame\n",
    "* Then you sequentially remove variables and test the scores, if it improves, then remove that variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Partial Dependence (@ 1:07:20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.1. Lecture Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technique is not very well-known but is very powerfull\n",
    "* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airdmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d07b0008baf96b10859126fdec4e07f87ea8cd98c5be91794f454136925afeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
