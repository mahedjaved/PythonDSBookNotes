{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great website for Textures :: https://pixela.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest in Depth - Chapter 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diving further into Random forests\n",
    "* For some datasets they work really well,  \n",
    "* What do we do when they don't work\n",
    "* What are the pros and cons of Random Forests\n",
    "* What parameters can be tuned \n",
    "* Look at how we interpret the results of random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FastAI library is highly **opinionated**\n",
    "* Meaning, majority of the code is **wrapped** around existing libraries\n",
    "* For example, for **structured** data there is already great code from the **scikit-learn** library\n",
    "* This is wrapped under fast.ai library --> with most emphasis on have functions that reproduce **SOTA results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"../fastai/\")\n",
    "from structured import *       \n",
    "from imports import *\n",
    "os.chdir(currDir)\n",
    "# ____________________________________________________________ #\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "# ____________________________________________________________ #\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2DATA = \"../datasets/kaggle/bluebook_bulldozers/\"\n",
    "# !dir PATH2DATA  # view what is in the data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1)**\n",
    "\n",
    "* Read the data and say which columns are dates\n",
    "* **Simlink** is used with the `ln -s` command\n",
    "* You can also use `sys.append` but not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f'{PATH2DATA}Train.csv',\n",
    "                     low_memory=False, parse_dates=[\"saledate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2)**\n",
    "\n",
    "* Now a better way of displaying the results \n",
    "* With the addittional property of setting the nos. of columns and rows to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000):\n",
    "        with pd.option_context(\"display.max_columns\", 1000):\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3)**\n",
    "\n",
    "* THen showing how only few of these last results can be displayed usign `tail()` method\n",
    "* But this data has too many rows, so it will be much better to show it columns x row wise using `transpose()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.tail().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4)**\n",
    "\n",
    "* You need to check what type of loss function is used to train the dataset\n",
    "* Can confirm it on Kaggle with the section on `evaluation`\n",
    "* Then, accordinly change the data\n",
    "* For example, Bluebook Bulldozers require `RMSLE` which means the dependant entries (in our case `Sale Price`) need to be `logged` as shown below\n",
    "* This is $\\sum (\\ln{(actual)} * \\ln{(preds)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.SalePrice = np.log(df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5)**\n",
    "\n",
    "* Initialise the random forest algorithm\n",
    "* Set it so that the dependant variable is Sale Price and all of the other columns except Sale Price are independant variables\n",
    "* The exception of such nature is carried out using the `df_raw.drop` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfalg = RandomForestRegressor(n_jobs=-1)\n",
    "# rfalg.fit(df_raw.drop(\"SalePrice\", axis=1), df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6)**\n",
    "\n",
    "* Make sure all the entries of tables are numbers\n",
    "* In particular `extract` further features from the `Sale Date` column entry\n",
    "* You can use `add_datepart` method in fast.ai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(df_raw, 'saledate')\n",
    "df_raw.saleYear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 7)**\n",
    "\n",
    "* Make sure all the categorical (`string types`) entries of tables are converted into numbers\n",
    "* You can use the `train_cat` or the `apply_cat` method in fast.ai library\n",
    "* Allows us to map categorical data to 0, 1, ... n\n",
    "* You can inspect this using the newly added `.cat.categories` method in fast.ai\n",
    "* And use `.cat.codes` to see all the mappings of the categorical entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats(df_raw)\n",
    "df_raw.UsageBand.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 8)**\n",
    "\n",
    "* Manually set the categories to improve RFAlgol performance slightly\n",
    "* This is done using the `.cat.set_categories` as endowed by the fast.ai libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.set_categories(\n",
    "    [\"High\", \"Medium\", \"Low\"], ordered=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 9)**\n",
    "\n",
    "* Remove any empty entries in the table\n",
    "* You can first inspect those using `display_all` and then calculating percentages of empty or NaN entries within the table\n",
    "* You can also use the `dtype` method to see that the type is now `Categorical` instead of a `string` which was its previous iteration\n",
    "* Laslty, you use the `proc_df` method to convert all of the empty and non-numeric keys to numeric ones, but before that save the old df_raw `feather` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_all(df_raw.isnull().sum().sort_index() / len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saving method I could'nt get it to work !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save using \"feather\" format\n",
    "# if not os.path.exists('../raw_saves/'):\n",
    "#     os.makedirs('../raw_saves', exist_ok=True)\n",
    "# df_raw.to_feather('../raw_saves/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `proc_df` to convert all missing and non-numeric columns to **numeric** ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y, _ = proc_df(df_raw, 'SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train on the Entire Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1)**\n",
    "\n",
    "* Finally prepare the Random Forest Algorithm\n",
    "* Fit and get the score for the performance on your data\n",
    "* Takes about 5mins on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfalg = RandomForestRegressor(n_jobs=-1)\n",
    "# rfalg.fit(df, y)\n",
    "# rfalg.score(df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max score getting is : `0.9881497324031044`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above is the `coefficient of determination`\n",
    "* Also known as the $R^2$ value\n",
    "* where ::\n",
    "    * $y_i$ are the ground truth variables\n",
    "    * $f_i$ are the model predictions (from RForest)\n",
    "    * $\\bar{y_i}$ are the Naive mean values\n",
    "\n",
    "\\begin{equation}\n",
    "    R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = \\frac{\\sum_i (y_i - f_i)^2}{\\sum_i (y_i - \\bar{y})^2} = 1 - \\frac{\\text{how good is your model}}{\\text{how good is the Naive model (just predict mean)}}\n",
    "\\end{equation}\n",
    "\n",
    "* Some golden rule to look out for:\n",
    "    * if `R=0` your model is as useless as the Naive model\n",
    "    * if `R>0` your model is worse than the Naive model\n",
    "    * if `R<0` your model is better than the Naive model\n",
    "\n",
    "--- \n",
    "\n",
    "* But a sensible range to assess the performance is **anything less than 1**\n",
    "* Why ? because you can tolerate 0 to 1, as long as you have an idea **how to improve** the model w.r.t Naive\n",
    "* But ideally, less than 0 is good, you can also have $1 - \\infty$ which again proves sensibility of the above anything less than 1 range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Training and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2_**\n",
    "\n",
    "* Have a validation set\n",
    "* Why because just because you have a good performance measure, doesnt mean that you will not suffer from **OVERFITTING**\n",
    "* How do you solve that ?\n",
    "* Sure, you just make a validation set\n",
    "* Before you make the validation set --> make sure that you come up with the dataset so that the score of your model on that dataset, should be good representation of how well your model will work in **real-life scenerio**\n",
    "* I.e. on Kaggle, outside online venues such as Kaggle e.g. in industry and so on\n",
    "* Often, in industry there is a bit of **disdain** against ML tech\n",
    "* Often saying that ML model was good in training, but was rubbish in application time\n",
    "* That's because their validation set was not **representative** of the real scenerio\n",
    "\n",
    "---\n",
    "\n",
    "* Another great hint is that the datasets with `temporal` information\n",
    "* For example Bulldozers have `Sale Price` at a particular time\n",
    "* So, in their dataset, they set the test set dates to have dates `in the future`\n",
    "* Which makes both their validation and test better `representative` of the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a, n):\n",
    "    \"\"\"\n",
    "    a: number of samples (i.e. the entire dataset)\n",
    "    n: number of training set to split\n",
    "    \"\"\"\n",
    "    # a[:n] will retrieve the first (N - n_valid) rows for TRAINING set\n",
    "    # a[n:] will retirve the last (N - n_valid) rows got VALIDATION set\n",
    "    return a[:n].copy(), a[n:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now make total of 12000 validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = 12000\n",
    "\n",
    "# the number of training sets will be len(df) - n_valid\n",
    "n_trn = len(df) - n_valid\n",
    "\n",
    "# now split the entire dataset into training and validation\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "\n",
    "# before were raw, now get the real ones based on pre-processed version\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {}, y_train shape : {},  x_valid shape : {}\".format(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `X_train` will be used to train the data\n",
    "* `x_valid` will be used to see how good it is working on isYOUR PORTION of the set\n",
    "* keep in mind the `test` set is UNKNOWN, yes it is a sub-set of the samples generated from the distribution that created the dataset, but the samples within it are unknown\n",
    "* If you do know, then you run into the issue of `overfitting` your model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Re_Train with Splitted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:**\n",
    "\n",
    "* Previously, we trained on the entire data\n",
    "* now re-train with training and validation set\n",
    "* setup the loss function needed to evaluate our model (**RMSE**)\n",
    "* make the prediction on certain bits of point in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(x, y):\n",
    "    return math.sqrt(((x - y) ** 2).mean())\n",
    "\n",
    "\n",
    "def print_scores(m):\n",
    "    result = [RMSE(m.predict(X_train), y_train), RMSE(m.predict(X_valid), y_valid),\n",
    "              m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "\n",
    "    # should also output \"oob_score\" if not then append to it\n",
    "    if hasattr(m, 'oob_score'):\n",
    "        result.append(m.oob_score)\n",
    "    print(\"[Rmse_on_X_train  :  RMSE_on_X_valid  :  Score_on_X_train  :  Score_on_X_valid]\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfalg = RandomForestRegressor(n_jobs=2)\n",
    "%time rfalg.fit(X_train, y_train)\n",
    "print_scores(rfalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU times: user 13min 42s, sys: 8.78 s, total: 13min 50s\n",
    "Wall time: 5min 26s\n",
    "[0.075577711024464, 0.23393271834047347, 0.9880622550698115, 0.9022695476674705]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:**\n",
    "\n",
    "* Basically speeding using `random sampling` of the data\n",
    "* Let's speed things up with using the `subset` argument in the `proc_df` function\n",
    "* And then train the model on the new samples again and compare the time\n",
    "* We dont want to store `X_val` because we dont want it to be overwritten, so that we make different models and compare their performance jointly\n",
    "* But we change `X_train` so that it doesnt overlap with the dates\n",
    "* If the above two do not happen then clearly we are cheating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, y_train, _ = proc_df(df_raw, 'SalePrice', subset=30000)\n",
    "X_train, _ = split_vals(df_train, 20000)\n",
    "y_train, _ = split_vals(y_train, 20000)\n",
    "\n",
    "# -------------------------------------------------------------- #\n",
    "\n",
    "rfalg = RandomForestRegressor(n_jobs=-1)\n",
    "%time rfalg.fit(X_train, y_train)\n",
    "print_scores(rfalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Asus Times] CPU times: user 26.2 s, sys: 216 ms, total: 26.4 s\n",
    "Wall time: 11.7 s\n",
    "[0.09430785246087922, 0.35056357955351314, 0.9808376570177445, 0.7805267658193739]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shiiitt, the time changed from **13min** to **26.2**, dayum !! that's FINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:**\n",
    "\n",
    "* Setup the RForestRegressor\n",
    "* And then find a single run\n",
    "* In Sklearn, these are called estimators, so we set that to 1 for `n_estimators` to get ouput from one tree\n",
    "* To stop the RForest algorithm to randomize the tree, we set `bootstrap` to False\n",
    "* This will make a small `deterministic` tree\n",
    "* Notice how score drops from 0.85 to 0.397\n",
    "* However, it is better than 0 so it is better than Naive ?? not sure, i think this statement is wrong !\n",
    "* It is not a good model, but it is model we can draw for sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfalg = RandomForestRegressor(n_estimators=1, \n",
    "    max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "rfalg.fit(X_train, y_train)\n",
    "print_scores(rfalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A tree consists of `binary decisions`\n",
    "* Otherwise known as `binary splits`\n",
    "* The `samples` entry found in the third point in each node, shows how many samples/rows are there for that split\n",
    "* The darker the colour of the node, the higher the value it has\n",
    "* The average of `log of price` is stored as `value` field\n",
    "* And if we build a model which uses the Naive method all the time, the `MSE` will store the loss\n",
    "* The first step to create the first tree, is to use the create the first `binary decision`\n",
    "* You can use `linear regression` to accomplish, but then again we are not assuming any `statistical assumption` \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A simpler method is to `choose variables` for each node (binary point)\n",
    "* And also choose what to split on\n",
    "* Split in for example decide from the population such that they are splitted `heterogenously`\n",
    "* Means the dividing sub-samples have different attributes from one another\n",
    "* Both each of the samples within the subset have `homoegenuous` properties\n",
    "* For each variable, see whether it is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(rfalg.estimators_[0], df_train, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bagging to Make Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The random forest is actually a way of `bagging` various decision trees\n",
    "* The idea of bagging is very similar to `ensembling`\n",
    "* Basically, make a whole lot of `big, deep, massively overfitting to problem` trees\n",
    "* But each one gets a different and `random` subset of data\n",
    "* The individual tree is perfect on that subset but crappy on the entire dataset\n",
    "* And do this x100 times\n",
    "* They all have errors, but the errors are `randomn` and `uncorrelated`\n",
    "* Which allows averaging of their predictions to get a `zero error` prediction that has the `combined insight` of all of the bagged trees\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:**\n",
    "\n",
    "* Re-run the forest algol but now with 10 `estimators` or `trees`\n",
    "* You decide the number of trees based on the `validation_rmse`\n",
    "* Sometimes `AutoML` such as `Amazon Sage` can help find the best Rforest for your problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfalg = RandomForestRegressor(n_jobs=-1)\n",
    "rfalg.fit(X_train, y_train)\n",
    "print_scores(rfalg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see the model is clearly overfitting on the training set since the score is 0.98 for `X_train` and 0.79 for `X_valid`\n",
    "* Much of the research focus is tied to making more uncorrelated trees as opposed to accurate correlated trees\n",
    "* The issue with making large number of trees that are trained on the same data is more likely to `NOT GENERALIZE` well , since most of the trees are predicting the same way\n",
    "* [OPTIONAL] There is also the `ExtraTreeRegressor` which can work here as well --> this does exactly the same as RForest, but rather than `trying ever split with every variable` approach as is with the Sklearn approach ...\n",
    "* ... it randomly tries with every variables, it is FASTER but probs. not as accurate\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:**\n",
    "\n",
    "* Now have a look and suspect some values output by one tree from the forest\n",
    "* We average all the predictions from the trees using the `np.mean(preds[:,0])` command\n",
    "* Remember that each tree is stored under the `rfalg.estimators_` command\n",
    "* As you can see from the results, the mean prediction was higher than the individual tree prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack([t.predict(X_valid) for t in rfalg.estimators_])\n",
    "print(\"Predictions:{}  Mean_Prediction:{}  y_valid:{}  RMSE_Between_Mean_And_X_Valid: {}\"\n",
    "    .format(preds[:,0], np.mean(preds[:,0]), y_valid[0], RMSE(np.mean(preds[:,0]), y_valid[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:**\n",
    "\n",
    "* Now do some plots of the mean prediction of trees\n",
    "* As we can see our model is doing better (*increasing $R^2$*) with the more `bagging` we do\n",
    "* Decide the number of trees by using the below graph\n",
    "* If the $R^2$ is improving then be sure to add more, if it flattens, **stop** right there!\n",
    "* You only need as many trees as you have time to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([metrics.r2_score(y_valid, np.mean(\n",
    "    preds[:i+1], axis=0)) for i in range(10)])\n",
    "plt.xlabel(\"Tree Number #\")\n",
    "plt.ylabel(\"$R^2$ Value\")\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('airdmp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d07b0008baf96b10859126fdec4e07f87ea8cd98c5be91794f454136925afeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
