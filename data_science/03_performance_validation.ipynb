{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Validation and Model Interpretation - Chapter 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To understand and `interpret` the predictive model\n",
    "* We demystify the idea that ML/DL models are `black-boxes`\n",
    "* Instead that RForest actual gives us useful `insights` regarding the data\n",
    "* We will also consider a larger dataset this chapter, particulary with over `1million` rows\n",
    "* This is Kaggle competition for `grocery forecasting`\n",
    "* Look at a model called `collaborative filtering`\n",
    "\n",
    "* Also learn a bit of tweaking today\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q) Question's been asked, how to choose ML models\n",
    "* A) For `unstructured` dataset, it is always good to use `deep learning` methods\n",
    "* S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Dataset Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading Third Party Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"../fastai/\")\n",
    "from structured import *       \n",
    "from imports import *\n",
    "os.chdir(currDir)\n",
    "# ____________________________________________________________ #\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "# ____________________________________________________________ #\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../datasets/kaggle/corporcion_favorita_grocery_sales/\"\n",
    "# !dir \"../datasets/kaggle/corporcion_favorita_grocery_sales/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Information on The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `dependant` variable is the one you are trying to **PREDICT**\n",
    "* In this dataset you are trying to predict ... \"How many `UNITS` of each kind of product was sold in `EACH STORE` on each day during the `two-week period`\n",
    "* The info that you want to predict is the \"How many `UNITS` each project at each store, on each day were sold in the last few years and for each store, date, product there is a metadata\n",
    "* `Metadata` based on the store includes information for example\n",
    "    > where is the store located\n",
    "    > what class of store is it \n",
    "* Meta data on the product type can include\n",
    "    > what was the oil price on this date ?\n",
    "    > what was the overall sales likes from the point of view of competitors ?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The grocery store dataset is a type of `Relational Dataset`\n",
    "* Meaning there are a number of different piece of information that we can `relate` together\n",
    "* This type of relational dataset is a type of `Star Schema`\n",
    "* A star schema is a kind of a `data warehousing` schema where we say there is some `central transaction`\n",
    "* You can think of this as star schema becaouse we can have a central transaction (i.e. the `train.csv`) and this branches out with different metadata based on targets such as `unit_sales`, `date`, `item_number` etc.\n",
    "* This is different to what is known as `Snowflake Schema`\n",
    "* Where there might be extra information available that may join targets across the central transaction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Importing and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:**\n",
    "\n",
    "* Begin with some basicimporting of the data\n",
    "* When using `pd.read_csv` if you say `limit_memory=False`, then we will set to use as much as memory as we like\n",
    "* This helps with figuring out what kind of data it is with more introspection possible\n",
    "* However, the system will run out of memory regardless of how big is your RAM\n",
    "* To limit the amount of memory to be used, we make a seperate columns of `types` of data we would like to store, this is demonstrated below\n",
    "* And as usual, you assign the column you would like to be parsed as dates in the `parse_dates` argument for which you pass the column name `[date]`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The logic behding chosing types is that the author is looking for the `smallest possible bits` needed to store the data\n",
    "* When working with large datasets, the `reading` and `writing` of the data is considerably slow\n",
    "* As a rule of thumb, `smaller datatypes` will RUN faster\n",
    "* In particularly if you use SIMD\n",
    "* SIMD, stands for `Single Instructure Multiple Data` vectorized code --> SIMD can pack more numbers into a single vector to `run at once`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.9 ms\n"
     ]
    }
   ],
   "source": [
    "types = {'id': 'int64',\n",
    "         'item_nbr': 'int32',\n",
    "         'store_nbr': 'int8',\n",
    "         'unit_sales': 'float32',\n",
    "         'onpromotion': 'object'}\n",
    "\n",
    "# below is only used when working on non_lenovo machine -- here, set chunksize=1000\n",
    "%time df_chunk = pd.read_csv(PATH+'train.csv', parse_dates=['date'], dtype=types, infer_datetime_format=True, chunksize=10000)\n",
    "df_all = df_chunk.get_chunk()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2**\n",
    "\n",
    "* Also, we set `onpromotion` to `object`\n",
    "* By default, the column onpromotion stores boolean variables\n",
    "* But we instead set its type to be `object`\n",
    "* Why ? because we need to pre-process it b4hand we name it as it has `missing values`\n",
    "* The pre-processing is done so as to avoid any gaps unexplainable to data holders or analytics\n",
    "* Keep in mind, setting to 'object' is not a good choice since it is a general purpose type which consumes `large amount of memory` and is `slow to use`\n",
    "* But it is the best we have so far\n",
    "* Now to fill all the missing values in the `onpromotion` columns with some binary values\n",
    "* After removing all the missing values, use the `.map` function to set all the `string booleans` to actual booleans\n",
    "* And then in the final line of code, convert it into a boolean\n",
    "* After the save file you can see that the data drops in memory from `train.csv` going from 4.65GB to the `tmp` file taking only 878MB\n",
    "* This saving memory technique allows us to inspect large scale datasets on less powerfull PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.onpromotion.fillna(False, inplace=True)\n",
    "df_all.onpromotion = df_all.onpromotion.map({'False': False, 'True': True})\n",
    "df_all.onpromotion = df_all.onpromotion.astype(bool)\n",
    "\n",
    "# save the temporary modified date\n",
    "# %time df_all.to_feather(PATH+'/tmp_grocery_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.7 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-02 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-02 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.006000</td>\n",
       "      <td>6.067551e+05</td>\n",
       "      <td>11.498948</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.284047</td>\n",
       "      <td>3.022362e+05</td>\n",
       "      <td>17.222500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.035010e+05</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.75000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.460650e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.838680e+05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7499.25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.645100e+05</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.118683e+06</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                 date     store_nbr      item_nbr  \\\n",
       "count   10000.00000                10000  10000.000000  1.000000e+04   \n",
       "unique          NaN                    2           NaN           NaN   \n",
       "top             NaN  2013-01-02 00:00:00           NaN           NaN   \n",
       "freq            NaN                 9422           NaN           NaN   \n",
       "first           NaN  2013-01-01 00:00:00           NaN           NaN   \n",
       "last            NaN  2013-01-02 00:00:00           NaN           NaN   \n",
       "mean     4999.50000                  NaN      6.006000  6.067551e+05   \n",
       "std      2886.89568                  NaN      5.284047  3.022362e+05   \n",
       "min         0.00000                  NaN      1.000000  1.035010e+05   \n",
       "25%      2499.75000                  NaN      3.000000  3.460650e+05   \n",
       "50%      4999.50000                  NaN      5.000000  5.838680e+05   \n",
       "75%      7499.25000                  NaN      7.000000  8.645100e+05   \n",
       "max      9999.00000                  NaN     25.000000  1.118683e+06   \n",
       "\n",
       "          unit_sales onpromotion  \n",
       "count   10000.000000       10000  \n",
       "unique           NaN           1  \n",
       "top              NaN        True  \n",
       "freq             NaN       10000  \n",
       "first            NaN         NaN  \n",
       "last             NaN         NaN  \n",
       "mean       11.498948         NaN  \n",
       "std        17.222500         NaN  \n",
       "min         0.252000         NaN  \n",
       "25%         3.000000         NaN  \n",
       "50%         6.000000         NaN  \n",
       "75%        13.000000         NaN  \n",
       "max       483.000000         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_all.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see first thing first, the dates look wrong in terms of format and has various NaNs there\n",
    "* Also we havent fixed the NaN values yet as well\n",
    "* So why formatting date is important ? --> because if you train your model at an earlier date and deploy it in a later date, your model should be `adaptable` enough to encorporate the changes\n",
    "* So you always need to make sure that in your data, the dates dont `overlap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:**\n",
    "\n",
    "* Repeat the same steps above but for the test sets\n",
    "* And always be on the look out for discrepancy between the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-16 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-16 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-16 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.254975e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>451994.296000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.888194e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208186.720121</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96995.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.254973e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>269286.750000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.254975e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>455733.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.254978e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>623320.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.254980e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>819209.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                 date  store_nbr       item_nbr  \\\n",
       "count   1.000000e+03                 1000     1000.0    1000.000000   \n",
       "unique           NaN                    1        NaN            NaN   \n",
       "top              NaN  2017-08-16 00:00:00        NaN            NaN   \n",
       "freq             NaN                 1000        NaN            NaN   \n",
       "first            NaN  2017-08-16 00:00:00        NaN            NaN   \n",
       "last             NaN  2017-08-16 00:00:00        NaN            NaN   \n",
       "mean    1.254975e+08                  NaN        1.0  451994.296000   \n",
       "std     2.888194e+02                  NaN        0.0  208186.720121   \n",
       "min     1.254970e+08                  NaN        1.0   96995.000000   \n",
       "25%     1.254973e+08                  NaN        1.0  269286.750000   \n",
       "50%     1.254975e+08                  NaN        1.0  455733.000000   \n",
       "75%     1.254978e+08                  NaN        1.0  623320.250000   \n",
       "max     1.254980e+08                  NaN        1.0  819209.000000   \n",
       "\n",
       "       onpromotion  \n",
       "count         1000  \n",
       "unique           2  \n",
       "top          False  \n",
       "freq           941  \n",
       "first          NaN  \n",
       "last           NaN  \n",
       "mean           NaN  \n",
       "std            NaN  \n",
       "min            NaN  \n",
       "25%            NaN  \n",
       "50%            NaN  \n",
       "75%            NaN  \n",
       "max            NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test = pd.read_feather(PATH+'tmp_test_grocery')\n",
    "df_test_chunk = pd.read_csv(\n",
    "    PATH+'test.csv', parse_dates=['date'], dtype=types, infer_datetime_format=True, chunksize=1000)\n",
    "\n",
    "df_test = df_test_chunk.get_chunk()\n",
    "\n",
    "df_test.onpromotion.fillna(False, inplace=True)\n",
    "df_test.onpromotion = df_test.onpromotion.map({'False': False, 'True': True})\n",
    "df_test.onpromotion = df_test.onpromotion.astype(bool)\n",
    "df_test.describe(include='all')\n",
    "\n",
    "# write to a file\n",
    "# df_test.to_feather(PATH+'tmp_test_grocery')\n",
    "\n",
    "# display the table\n",
    "df_test.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you can clearly see that in the test set notice the dates begin one day later form the training set\n",
    "* So then your model should be able to `forecast` based on the date you are given\n",
    "* This is fundamental level of ML that all should know, the test set must **TEST** the ability of the model to forecast\n",
    "* Instead of randomly sampling, why not look at the latest dates in the test set using `.tail()`, think about this, you need to be able to predict on the latest information and your model should be able to predict on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>9</td>\n",
       "      <td>698643</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>9</td>\n",
       "      <td>716241</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>9</td>\n",
       "      <td>716242</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>9</td>\n",
       "      <td>716245</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>9</td>\n",
       "      <td>716250</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       date  store_nbr  item_nbr  unit_sales  onpromotion\n",
       "9995  9995 2013-01-02          9    698643        20.0         True\n",
       "9996  9996 2013-01-02          9    716241         5.0         True\n",
       "9997  9997 2013-01-02          9    716242        12.0         True\n",
       "9998  9998 2013-01-02          9    716245         7.0         True\n",
       "9999  9999 2013-01-02          9    716250        10.0         True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:**\n",
    "\n",
    "* Next we demonstrate how to load from the saved temporary folder and set that to the `df_all` variable\n",
    "* So that there is no overlap between the one loaded from the original .csv file\n",
    "* And the `truncated` version we just made\n",
    "* And now we take the `log` of the sales, just like in previous data\n",
    "* This the dependant variable remember, we are trying to predict the sales, and we want it in logs so we can predict something that `varies according to ratios` and the loss function will again be `RMLSE`\n",
    "* Also always be attentive as to what the project description is saying\n",
    "* For example, in grocery sales it says that the `negative sales` should be counted as `zeros`\n",
    "* So we `clip` the sales so they fall between `0` and `None`, where none means undefined maximum val\n",
    "* The usage of $\\ln(sales) + 1$ is also there as per suggestion of the project description $\\rightarrow$ hence why we use `np.log1p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.unit_sales = np.log1p(np.clip(df_all.unit_sales, 0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5:**\n",
    "\n",
    "* Now we resume with the pre-processing of dates as carried out in previous databases\n",
    "* We again use the `add_date_part` function provided by the Fast.AI library\n",
    "* Usually, you'd run with a smaller subsample to make sure your function runs correctly\n",
    "* Also, we dont use `train_cats` here because all the columns are numeric\n",
    "* We do, however, need to run `proc_df` on the target/dependant variable `Unit_Sales` for appending missing values to numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%time add_datepart(df_all, 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6:**\n",
    "\n",
    "* Here comes the usual split again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape: (9000, 18)  Validset shape: (1000, 18)\n"
     ]
    }
   ],
   "source": [
    "def split_values(a, n):\n",
    "    return a[:n].copy(), a[n:].copy()\n",
    "    \n",
    "n_valid = len(df_test)\n",
    "n_trn = len(df_all) - n_valid\n",
    "train, valid = split_values(df_all, n_trn)\n",
    "print(\"Trainset shape: {}  Validset shape: {}\".format(train.shape, valid.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 7:**\n",
    "\n",
    "* Run the `proc_df` function for missing values replacement to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.67 ms\n"
     ]
    }
   ],
   "source": [
    "%time trn, y, _ = proc_df(train, 'unit_sales')\n",
    "val, y_val, _ = proc_df(valid, 'unit_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Algorithm Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:**\n",
    "\n",
    "* We still care about using *rmse* as measure of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "    return math.sqrt(((x - y)**2).mean())\n",
    "\n",
    "\n",
    "def print_score(m, x, y, val=val, y_val=y_val):\n",
    "    res = [rmse(m.predict(x), y), rmse(m.predict(val), y_val),\n",
    "           m.score(x, y), m.score(val, y_val)]\n",
    "    if hasattr(m, 'oob_score_'):\n",
    "        res.append(m.oob_score_)\n",
    "    print(\"RMSE_X_train  :  RMSE_X_valid  :  Score_X_train  :  Score_X_valid\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:**\n",
    "\n",
    "* Now we look back at the function `set_rf_samples`\n",
    "* 1st consider the background, we have about 1e6 samples, and we dont want to create a tree with 120million records\n",
    "* So instead start with 10,000 or 100,000 according to author, using 1million (1,000,000) runs within under a minute\n",
    "* Hence why we use set_rf_samples=1million, for Lenovo I used `1000`, maybe `100` for Asus or Dell\n",
    "* Also it completely never came across my notice that you could use underscore to set large numbers in Python ... Wow !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also author converted datatypes to floats\n",
    "* Why ? because save time, it is done anyways in the internal pandas libraries, however, doing it seperately and only once, saves you a couple of minutes\n",
    "* You can also use the magic command `%prun` to be able to profile the code and see which line takes the most amount of time in running\n",
    "* You can practice this on any line of code that is taking more than 20secons and inspect which line needs to be optimised\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also you cannot use `oob_score` if you run `rf_sample` so instead I would use `chunksize` in order to section the data \n",
    "* Alternatively, you have the option of manually writing in the oob_score as well\n",
    "* But author said it is NOT recommended for large datasets, for this calculating the oob_score will take too much time\n",
    "* You can also inspect if oob_score is available to you using the command `rfalg.oob_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "# not sure if you need this if you use chunksize\n",
    "# set_rf_samples(1_000)  \n",
    "%time x = np.array(trn, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:**\n",
    "\n",
    "* Finally create the random forrest and fit it with at least `20 estimators/trees`\n",
    "* The number of jobs is equal to number of cores, setting `n_jobs=-1` means use every single core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 118 ms\n",
      "RMSE_X_train  :  RMSE_X_valid  :  Score_X_train  :  Score_X_valid\n",
      "[0.8434397863716047, 0.9453934295747514, 0.13949736216972342, 0.00719903865985505]\n"
     ]
    }
   ],
   "source": [
    "rfalg = RandomForestRegressor(n_estimators=20, min_samples_leaf=100, n_jobs=2)\n",
    "%time rfalg.fit(x, y)\n",
    "print_score(rfalg, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:**\n",
    "\n",
    "* Improving further (reducing the error further) by reducing `min_samples_leaf`\n",
    "* In this example we drop this from 100 to 10\n",
    "* We clearly saw a jump in score on both x_train and x_valid from `0.139` to `0.312` when reducing min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 218 ms\n",
      "RMSE_X_train  :  RMSE_X_valid  :  Score_X_train  :  Score_X_valid\n",
      "[0.7551882462056887, 0.9138796843603745, 0.3101502950698689, 0.07228393813131406]\n"
     ]
    }
   ],
   "source": [
    "rfalg = RandomForestRegressor(n_estimators=20, min_samples_leaf=10, n_jobs=2)\n",
    "%time rfalg.fit(x, y)\n",
    "print_score(rfalg, x, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now keep decreasing min_samples_leaf down to 3 and observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 239 ms\n",
      "RMSE_X_train  :  RMSE_X_valid  :  Score_X_train  :  Score_X_valid\n",
      "[0.6084471930368329, 0.9121156036047839, 0.5521940881441207, 0.07586206013318464]\n"
     ]
    }
   ],
   "source": [
    "rfalg = RandomForestRegressor(n_estimators=20, min_samples_leaf=3, n_jobs=2)\n",
    "%time rfalg.fit(x, y)\n",
    "print_score(rfalg, x, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discussion on Limitations of Random Forest Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RForest only knows how to make splits at various columns\n",
    "* It doesnt know where the location of store is\n",
    "* It has a hunch on the correlation and then it makes the splits\n",
    "* Coding for ML is very difficult, small changes in details you get bad performance\n",
    "* And if you are NOT on kaggle you wont be able to tell where you are wrong\n",
    "* Debugging is very intricate in ML programming\n",
    "* Also, Might not be very good for RForest to work based on very old as in 4 years of data, so Kaggle offers kernel that follows that: takes the last two weeks and take the average sales by `date`, `store_number`, `on_promotion` and take mean across date just submit that -=> and you get 30th position\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ways to Improve Performance on this Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always remeber the STAR schema : the `supplementary` data will always help the data organisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Student A` suggestion : model seasonality and trend effects in different column e.g. average sales per month\n",
    "* `Jeremy` suggestion : finish testing a model and train a new and draw a scatter plot (x_axis predictions of old model Vs predictions of new model) they should form a line and if it doesnt then you have screwed sth up. ALso check upon `Rossman challenge` from Kaggle, it is very relatable to Ecuador's grocery challenge\n",
    "* `Student B` suggestion : consider Ecuador's holidays "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Advice on Bad Validation Set Cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When you have bad valid_set, it can be difficult to make a good model out of it\n",
    "* Validation set must be reliable : i.e. it must be able to tell if your model is going to do well in production case or not\n",
    "* You can use the test to calibrate your validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../fastai/images/terrence_validation.png' height=300px width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airdmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d07b0008baf96b10859126fdec4e07f87ea8cd98c5be91794f454136925afeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
