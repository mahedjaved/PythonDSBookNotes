{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Validation and Model Interpretation - Chapter 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To understand and `interpret` the predictive model\n",
    "* We demystify the idea that ML/DL models are `black-boxes`\n",
    "* Instead that RForest actual gives us useful `insights` regarding the data\n",
    "* We will also consider a larger dataset this chapter, particulary with over `1million` rows\n",
    "* This is Kaggle competition for `grocery forecasting`\n",
    "* Look at a model called `collaborative filtering`\n",
    "* Also learn a bit of tweaking today\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q) Question's been asked, how to choose ML models\n",
    "* A) For `unstructured` dataset, it is always good to use `deep learning` methods\n",
    "* S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Dataset Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading Third Party Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplotliba` not found.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotliba inlinae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"../fastai/\")\n",
    "from structured import *       \n",
    "from imports import *\n",
    "os.chdir(currDir)\n",
    "# ____________________________________________________________ #\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "# ____________________________________________________________ #\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../datasets/kaggle/corporcion_favorita_grocery_sales/\"\n",
    "# !dir \"../datasets/kaggle/corporcion_favorita_grocery_sales/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Information on The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `dependant` variable is the one you are trying to **PREDICT**\n",
    "* In this dataset you are trying to predict ... \"How many `UNITS` of each kind of product was sold in `EACH STORE` on each day during the `two-week period`\n",
    "* The info that you want to predict is the \"How many `UNITS` each project at each store, on each day were sold in the last few years and for each store, date, product there is a metadata\n",
    "* `Metadata` based on the store includes information for example\n",
    "    > where is the store located\n",
    "    > what class of store is it \n",
    "* Meta data on the product type can include\n",
    "    > what was the oil price on this date ?\n",
    "    > what was the overall sales likes from the point of view of competitors ?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The grocery store dataset is a type of `Relational Dataset`\n",
    "* Meaning there are a number of different piece of information that we can `relate` together\n",
    "* This type of relational dataset is a type of `Star Schema`\n",
    "* A star schema is a kind of a `data warehousing` schema where we say there is some `central transaction`\n",
    "* You can think of this as star schema becaouse we can have a central transaction (i.e. the `train.csv`) and this branches out with different metadata based on targets such as `unit_sales`, `date`, `item_number` etc.\n",
    "* This is different to what is known as `Snowflake Schema`\n",
    "* Where there might be extra information available that may join targets across the central transaction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Importing and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:**\n",
    "\n",
    "* Begin with some basicimporting of the data\n",
    "* When using `pd.read_csv` if you say `limit_memory=False`, then we will set to use as much as memory as we like\n",
    "* This helps with figuring out what kind of data it is with more introspection possible\n",
    "* However, the system will run out of memory regardless of how big is your RAM\n",
    "* To limit the amount of memory to be used, we make a seperate columns of `types` of data we would like to store, this is demonstrated below\n",
    "* And as usual, you assign the column you would like to be parsed as dates in the `parse_dates` argument for which you pass the column name `[date]`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The logic behding chosing types is that the author is looking for the `smallest possible bits` needed to store the data\n",
    "* When working with large datasets, the `reading` and `writing` of the data is considerably slow\n",
    "* As a rule of thumb, `smaller datatypes` will RUN faster\n",
    "* In particularly if you use SIMD\n",
    "* SIMD, stands for `Single Instructure Multiple Data` vectorized code --> SIMD can pack more numbers into a single vector to `run at once`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "types = {'id': 'int64',\n",
    "         'item_nbr': 'int32',\n",
    "         'store_nbr': 'int8',\n",
    "         'unit_sales': 'float32',\n",
    "         'onpromotion': 'object'}\n",
    "\n",
    "%time\n",
    "# df_all = pd.read_csv(\n",
    "#     PATH+'train.csv', parse_dates=['date'], dtype=types, infer_datetime_format=True)\n",
    "df_all = pd.read_feather(PATH+'tmp_all_grocery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2**\n",
    "\n",
    "* Also, we set `onpromotion` to `object`\n",
    "* By default, the column onpromotion stores boolean variables\n",
    "* But we instead set its type to be `object`\n",
    "* Why ? because we need to pre-process it b4hand we name it as it has `missing values`\n",
    "* The pre-processing is done so as to avoid any gaps unexplainable to data holders or analytics\n",
    "* Keep in mind, setting to 'object' is not a good choice since it is a general purpose type which consumes `large amount of memory` and is `slow to use`\n",
    "* But it is the best we have so far\n",
    "* Now to fill all the missing values in the `onpromotion` columns with some binary values\n",
    "* After removing all the missing values, use the `.map` function to set all the `string booleans` to actual booleans\n",
    "* And then in the final line of code, convert it into a boolean\n",
    "* After the save file you can see that the data drops in memory from `train.csv` going from 4.65GB to the `tmp` file taking only 878MB\n",
    "* This saving memory technique allows us to inspect large scale datasets on less powerfull PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.onpromotion.fillna(False, inplace=True)\n",
    "# df_all.onpromotion = df_all.onpromotion.map({'False': False, 'True': True})\n",
    "# df_all.onpromotion = df_all.onpromotion.astype(bool)\n",
    "\n",
    "# save the temporary modified date\n",
    "# %time df_all.to_feather(PATH+'/tmp_grocery_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>125497040</td>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>125497040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>118194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96028767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-15 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.274852e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.746458e+01</td>\n",
       "      <td>9.727692e+05</td>\n",
       "      <td>8.554856e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.622788e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.633051e+01</td>\n",
       "      <td>5.205336e+05</td>\n",
       "      <td>2.360515e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.699500e+04</td>\n",
       "      <td>-1.537200e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.137426e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>5.223830e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.274852e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>9.595000e+05</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.412278e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>1.354380e+06</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.127114e+06</td>\n",
       "      <td>8.944000e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                 date     store_nbr      item_nbr  \\\n",
       "count   1.254970e+08            125497040  1.254970e+08  1.254970e+08   \n",
       "unique           NaN                 1684           NaN           NaN   \n",
       "top              NaN  2017-07-01 00:00:00           NaN           NaN   \n",
       "freq             NaN               118194           NaN           NaN   \n",
       "first            NaN  2013-01-01 00:00:00           NaN           NaN   \n",
       "last             NaN  2017-08-15 00:00:00           NaN           NaN   \n",
       "mean    6.274852e+07                  NaN  2.746458e+01  9.727692e+05   \n",
       "std     3.622788e+07                  NaN  1.633051e+01  5.205336e+05   \n",
       "min     0.000000e+00                  NaN  1.000000e+00  9.699500e+04   \n",
       "25%     3.137426e+07                  NaN  1.200000e+01  5.223830e+05   \n",
       "50%     6.274852e+07                  NaN  2.800000e+01  9.595000e+05   \n",
       "75%     9.412278e+07                  NaN  4.300000e+01  1.354380e+06   \n",
       "max     1.254970e+08                  NaN  5.400000e+01  2.127114e+06   \n",
       "\n",
       "          unit_sales onpromotion  \n",
       "count   1.254970e+08   125497040  \n",
       "unique           NaN           2  \n",
       "top              NaN       False  \n",
       "freq             NaN    96028767  \n",
       "first            NaN         NaN  \n",
       "last             NaN         NaN  \n",
       "mean    8.554856e+00         NaN  \n",
       "std     2.360515e+01         NaN  \n",
       "min    -1.537200e+04         NaN  \n",
       "25%     2.000000e+00         NaN  \n",
       "50%     4.000000e+00         NaN  \n",
       "75%     9.000000e+00         NaN  \n",
       "max     8.944000e+04         NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_all.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see first thing first, the dates look wrong in terms of format and has various NaNs there\n",
    "* Also we havent fixed the NaN values yet as well\n",
    "* So why formatting date is important ? --> because if you train your model at an earlier date and deploy it in a later date, your model should be `adaptable` enough to encorporate the changes\n",
    "* So you always need to make sure that in your data, the dates dont `overlap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:**\n",
    "\n",
    "* Repeat the same steps above but for the test sets\n",
    "* And always be on the look out for discrepancy between the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.370464e+06</td>\n",
       "      <td>3370464</td>\n",
       "      <td>3.370464e+06</td>\n",
       "      <td>3.370464e+06</td>\n",
       "      <td>3370464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-16 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>210654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3171867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-16 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.271823e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.750000e+01</td>\n",
       "      <td>1.244798e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.729693e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.558579e+01</td>\n",
       "      <td>5.898362e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.699500e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.263397e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>8.053210e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.271823e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.750000e+01</td>\n",
       "      <td>1.294665e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.280249e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>1.730015e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.288675e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.134244e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                 date     store_nbr      item_nbr  \\\n",
       "count   3.370464e+06              3370464  3.370464e+06  3.370464e+06   \n",
       "unique           NaN                   16           NaN           NaN   \n",
       "top              NaN  2017-08-16 00:00:00           NaN           NaN   \n",
       "freq             NaN               210654           NaN           NaN   \n",
       "first            NaN  2017-08-16 00:00:00           NaN           NaN   \n",
       "last             NaN  2017-08-31 00:00:00           NaN           NaN   \n",
       "mean    1.271823e+08                  NaN  2.750000e+01  1.244798e+06   \n",
       "std     9.729693e+05                  NaN  1.558579e+01  5.898362e+05   \n",
       "min     1.254970e+08                  NaN  1.000000e+00  9.699500e+04   \n",
       "25%     1.263397e+08                  NaN  1.400000e+01  8.053210e+05   \n",
       "50%     1.271823e+08                  NaN  2.750000e+01  1.294665e+06   \n",
       "75%     1.280249e+08                  NaN  4.100000e+01  1.730015e+06   \n",
       "max     1.288675e+08                  NaN  5.400000e+01  2.134244e+06   \n",
       "\n",
       "       onpromotion  \n",
       "count      3370464  \n",
       "unique           2  \n",
       "top          False  \n",
       "freq       3171867  \n",
       "first          NaN  \n",
       "last           NaN  \n",
       "mean           NaN  \n",
       "std            NaN  \n",
       "min            NaN  \n",
       "25%            NaN  \n",
       "50%            NaN  \n",
       "75%            NaN  \n",
       "max            NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test = pd.read_csv(\n",
    "#     PATH+'test.csv', parse_dates=['date'], dtype=types, infer_datetime_format=True)\n",
    "\n",
    "# df_test.onpromotion.fillna(False, inplace=True)\n",
    "# df_test.onpromotion = df_test.onpromotion.map({'False' : False, 'True' : True})\n",
    "# df_test.onpromotion = df_test.onpromotion.astype(bool)\n",
    "# df_test.describe(include='all')\n",
    "\n",
    "df_test.to_feather(PATH+'tmp_test_grocery')\n",
    "df_test.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you can clearly see that in the test set notice the dates begin one day later form the training set\n",
    "* So then your model should be able to `forecast` based on the date you are given\n",
    "* This is fundamental level of ML that all should know, the test set must **TEST** the ability of the model to forecast\n",
    "* Instead of randomly sampling, why not look at the latest dates in the test set using `.tail()`, think about this, you need to be able to predict on the latest information and your model should be able to predict on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125497035</th>\n",
       "      <td>125497035</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2089339</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497036</th>\n",
       "      <td>125497036</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2106464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497037</th>\n",
       "      <td>125497037</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2110456</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497038</th>\n",
       "      <td>125497038</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2113914</td>\n",
       "      <td>198.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497039</th>\n",
       "      <td>125497039</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2116416</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id       date  store_nbr  item_nbr  unit_sales  onpromotion\n",
       "125497035  125497035 2017-08-15         54   2089339         4.0        False\n",
       "125497036  125497036 2017-08-15         54   2106464         1.0         True\n",
       "125497037  125497037 2017-08-15         54   2110456       192.0        False\n",
       "125497038  125497038 2017-08-15         54   2113914       198.0         True\n",
       "125497039  125497039 2017-08-15         54   2116416         2.0        False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:**\n",
    "\n",
    "* Next we demonstrate how to load from the saved temporary folder and set that to the `df_all` variable\n",
    "* So that there is no overlap between the one loaded from the original .csv file\n",
    "* And the `truncated` version we just made\n",
    "* And now we take the `log` of the sales, just like in previous data\n",
    "* This the dependant variable remember, we are trying to predict the sales, and we want it in logs so we can predict something that `varies according to ratios` and the loss function will again be `RMLSE`\n",
    "* Also always be attentive as to what the project description is saying\n",
    "* For example, in grocery sales it says that the `negative sales` should be counted as `zeros`\n",
    "* So we `clip` the sales so they fall between `0` and `None`, where none means undefined maximum val\n",
    "* The usage of $\\ln(sales) + 1$ is also there as per suggestion of the project description $\\rightarrow$ hence why we use `np.log1p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.unit_sales = np.log1p(np.clip(df_all.unit_sales, 0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5:**\n",
    "\n",
    "* Now we resume with the pre-processing of dates as carried out in previous databases\n",
    "* We again use the `add_date_part` function provided by the Fast.AI library\n",
    "* Usually, you'd run with a smaller subsample to make sure your function runs correctly\n",
    "* Also, we dont use `train_cats` here because all the columns are numeric\n",
    "* We do, however, need to run `proc_df` on the target/dependant variable `Unit_Sales` for appending missing values to numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time add_datepart(df_all, 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6:**\n",
    "\n",
    "* Here comes the usual split again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape: (122126576, 18)  Validset shape: (3370464, 18)\n"
     ]
    }
   ],
   "source": [
    "def split_values(a, n):\n",
    "    return a[:n].copy(), a[n:].copy()\n",
    "    \n",
    "n_valid = len(df_test)\n",
    "n_trn = len(df_all) - n_valid\n",
    "train, valid = split_values(df_all, n_trn)\n",
    "print(\"Trainset shape: {}  Validset shape: {}\".format(train.shape, valid.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 7:**\n",
    "\n",
    "* Run the `proc_df` function for missing values replacement to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "trn, y, _ = proc_df(train, 'unit_sales')\n",
    "val, y_val, _ = proc_df(valid, 'unit_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "    return math.sqrt(((x - y)**2).mean())\n",
    "\n",
    "\n",
    "def print_score(m, x, y, val=val, y_val=y_val):\n",
    "    res = [rmse(m.predict(x), y), rmse(m.predict(val), y_val),\n",
    "           m.score(x, y), m.score(val, y_val)]\n",
    "    if hasattr(m, 'oob_score_'):\n",
    "        res.append(m.oob_score_)\n",
    "    print(\"RMSE_X_train  :  RMSE_X_valid  :  Score_X_train  :  Score_X_valid\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('airdmp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d07b0008baf96b10859126fdec4e07f87ea8cd98c5be91794f454136925afeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
